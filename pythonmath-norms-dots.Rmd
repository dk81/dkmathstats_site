---
title: "Computing Norms & Dot Products In Python"
output: html_document
---

&nbsp;

Hello there. This post is a follow up post to my previous post on norms and dot products. Examples of computing norms and dot products in the programming languages R and Python will be shown.

&nbsp;

### <u>The Demand For Computing</u>

When you start learning mathematics, you learn the theory first for understanding and practice simple examples with pen and paper. With technology being more and more powerful and accessible, computational mathematics has become more popular. Why compute norms of vectors in the hundredth dimension $\mathbb{R}^{100}$ by hand when it is easier and faster to do so with software.

&nbsp;

### <u>Norms and Dot Products in Python and R</u>

Even though I know R much more than Python, Python was the first programming language I learned when I took an Introduction to Programming course. MATLAB was another program I used from time to time but with Numpy and similar packages in Python, Python has more utility than MATLAB.

As someone who switched from mathematics to applied statistics, I use R very often.

In each example, the first set of code will be Python code and the second set of code will be code in R.

To import the Numpy package in Python to enable linear algebra functions we type in:

&nbsp;

```{python}
import numpy as np
```

&nbsp;

---

### <u>Norms</u>

&nbsp;

Given the vector $\textbf{x} = (-3, 4)$, the norm of $\textbf{x}$ is $||\textbf{x}|| = 5$. It can be computed as follows.

In Python:

&nbsp;

```{python}
## Norms:

x = np.array([-3, 4])

print(np.linalg.norm(x))


```

&nbsp;

In R:

&nbsp;

```{r}
# a)

a <- c(-3, 4)
 
# Two ways of computing a norm:

norm(a, type = "2")


sqrt(sum(a^2))
```

&nbsp;

---

If the vector $\textbf{y}$ in $\mathbb{R}^{4}$ is $\textbf{y} = (5, 1, 10, -2)$, the norm would be $||\textbf{y}|| \approx 11.401$.

In Python:

&nbsp;

```{python}
y = np.array([5, 1, 10, -2])

print(np.linalg.norm(y))
```

&nbsp;

In R:

&nbsp;

```{r, echo = TRUE}
b <- c(5, 1, 10, -2)

norm(b, type = "2")


sqrt(sum(b^2))

```

&nbsp;

With the Python code, we express the vector as an array in the Numpy package and use the norm function to compute norms of vectors.

To create vectors in R, the `c()` function is used.

In R, there are two ways of computing norms (that I know of). The first one uses the norm function with type = "2". (I don't know why type 2, check the reference or help.) The second way is more direct and aligns with the mathematical definition of a norm. We squared each element in the vector, take the sum of squares and square root that sum.

&nbsp;

---

&nbsp;

**Distance Between Two Vectors**

&nbsp;

Recall that in $\mathbb{R}^{n}$, the distance between vectors $\textbf{a} = (a_1, a_2, a_3, \dots, a_n)$ and $\textbf{b} = (b_1, b_2, b_3, \dots, b_n)$ is

&nbsp;

$$\displaystyle d(\textbf{a}, \textbf{b}) = ||\textbf{a} - \textbf{b}|| = \sqrt{ (a_1 - b_1)^2 + (a_2 - b_2)^2 + (a_3 - b_3)^2 + \dots + (a_n - b_n)^2}$$

&nbsp;

To compute distance between vectors we do subtraction element-wise, square the results, sum the differences of squares and take the square root of the sum.

If we were to calculate the distance between the vectors $\textbf{v} = (10, 5, -2, -1)$ and $\textbf{w} = (-1, 0, 2, 1)$ in $\mathbb{R}^{4}$, we would use the distance formula. The answer is around 12.884. The code is as follows.

In Python:

&nbsp;

```{python}
v = np.array([10, 5, -2, -1])
w = np.array([-1, 0, 2, 1])

print(np.linalg.norm(v - w))
```

&nbsp;

In R:

```{r}
## Distance Between Two Vectors:

v <- c(10, 5, -2, -1)
w <- c(-1, 0, 2, 1)

# Three ways of computing distance between vectors in R:

sqrt(sum((v - w)^2))


dist(rbind(v, w))


norm(v - w, type = "2")

```

&nbsp;

The coding method is similar as before. The second way in R is strange but it works. (See one of the Reference links)

---

### <u>Dot Products</u>

&nbsp;

These examples for the dot products are the same examples in my previous post.

&nbsp;

**Example One**

The dot product of vectors $\textbf{a} = (1, -2)$ and $\textbf{b} = (-10, -3)$ is -4.

In Python:

&nbsp;

```{python}
a = np.array([1, -2])
b = np.array([-10, -3])

print(np.dot(a, b))

```

&nbsp;

In R:

```{r}
# Assuming equal lengths of vector the multiplication operator in R does
# element wise multiplication (i.e. a1 * b1).

# Example 1:

a <- c(1, -2)
b <- c(-10, -3)
sum(a * b)

```

&nbsp;

**Example Two**

Given vectors $\textbf{c} = (1, -2, 5, 3)$ and $\textbf{d} = (4, -3, -7, 2)$, the dot product of these vectors in $\mathbb{R}^{4}$ is -19.


In Python:

```{python}
# Example Two:

c = np.array([1, -2, 5, 3])
d = np.array([4, -3, -7, 2])

print(np.dot(c, d))

```

In R:

```{r}
# Example 2:

c <- c(1, -2, 5, 3)
d <- c(4, -3, -7, 2)

sum(c * d)
```

&nbsp;

**Example Three**

Suppose we are given the vectors $\textbf{e} = (3, -5)$, $\textbf{f} = (-1, 4)$ and $\textbf{g} = (3, 2)$. Evaluating $(\textbf{e} + \textbf{g}) \cdot \textbf{f}$ gives us -18.

```
In Python:

# Example Three: (e + g) * f

e = np.array([3, -5])
f = np.array([-1, 4])
g = np.array([3, 2])

print(np.dot(e + g, f))

-18
```

&nbsp;

In R:

&nbsp;

```{r}
# Example 3: (e + g) * f

e <- c(3, -5)
f <- c(-1, 4)
g <- c(3, 2)

sum((e + g) * f)
```

&nbsp;

Computing the dot product in Python and Numpy is pretty straight forward.

Computing the dot product in R involves using element wise multiplication and then taking the sum.

&nbsp;

### <u>Notes</u>

&nbsp;

We refer to the norm as the Euclidean norm. There are other norms and other distance measures available but those are not discussed here.

The decision between using R, Python or another language such as C++ is up to the user. I prefer R as I am used to it and I like the utility it provides. (Maybe eventually I will get more used to Python.)

---

### <u>References</u>

&nbsp;

These particular web links were useful.

* http://stackoverflow.com/questions/5559384/euclidean-distance-of-two-vectors

* http://stackoverflow.com/questions/10933945/how-to-calculate-the-euclidean-norm-of-a-vector-in-r

The featured image is from http://www.npl.co.uk/upload/img/maths-header.jpg.



