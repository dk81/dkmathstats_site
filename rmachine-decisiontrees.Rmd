---
title: "Decision Trees In R"
output: html_document
---

&nbsp;

Hi there. In this post, I share some experimental work that I have done with decision trees in R.



&nbsp;

### <u>Sections</u>

&nbsp;

* <a href="#overview">Decision Trees Overview</a>
* <a href="#one">Example One - Grades Dataset</a>
* <a href="#two">Example Two - Wine Quality Data</a>
* <a href="#refs">References</a>


<a name="overview"></a>

&nbsp;

### <u>Decision Trees Overview</u>

&nbsp;

Decision trees are used as a tool for making predictions on categorical variables. These trees also give information on decision options and the criteria involved in these decisions. They appear in a flow chart type of fashion that is easy to follow for the viewer.

An example of a decision tree is shown below.

&nbsp;

<center>![](./images/decision-tree-example.png)</center>

&nbsp;

Image Source: https://databricks.com/wp-content/uploads/2014/09/decision-tree-example.png


<a name="one"></a>

&nbsp;

### <u>Example One - A Small Grades Dataset</u>

&nbsp;

In this example, I work with a small dataset from R's `faraway` library. This dataset is called `spector` and it investigates whether a student's grade in an economics class has improved under a new teaching method. Here is an image of the documentation (or use `??faraway` and click on spector).

&nbsp;

<center>![](./econ_data_doc.jpg)</center>

&nbsp;

In R, I load in the faraway package and take a look at the data with the `head()`, `tail()` and `str()` functions.

&nbsp;

```{r, message=FALSE, warning=FALSE}
# Decision Trees In R:

# References:

# https://www.tutorialspoint.com/r/r_decision_tree.htm
# https://www.r-bloggers.com/a-brief-tour-of-the-trees-and-forests/
# https://www.youtube.com/watch?v=JFJIQ0_2ijg
# http://trevorstephens.com/kaggle-titanic-tutorial/r-part-3-decision-trees/

# Load packages:

library(faraway)
library(party)
library(rpart)

grades_data <- spector

# Preview data:

head(grades_data)

tail(grades_data)

str(grades_data)

```

&nbsp;

I test out this decision tree from the `ctree()` function.

&nbsp;

```{r, echo = TRUE, fig.width=6, fig.height=6} 
# Create decision treeby using ctree(). Did the student's grade improved?

tree_output <- ctree(formula = grade ~ psi + tuce + gpa, 
                     data = grades_data)



plot(tree_output)
```

&nbsp;

This output does not look great nor informative. The `ctree()` function is not recommended in my opinion.

I then try making a decision tree with the rpart package in R. Here is what I got.

&nbsp;

```{r, echo = TRUE, fig.width=7, fig.height=7}
## Using rpart package:

model_fit <- rpart(formula = grade ~ psi + tuce + gpa, method = "class", 
                   data = grades_data)

# Plot tree
plot(model_fit,  uniform = TRUE, main = "Classification Tree For Grade Improvements")
text(model_fit, use.n = TRUE, all=TRUE, cex = .8)
```

&nbsp;

For some reason, the output got cutoff in my Rstudio program. A different tool would be needed.

What worked for me was using the rpart and rattle packages together to create a good decision tree with colour. The `fancyRpartPlot()` function from the rattle package does the trick.

&nbsp;

```{r, echo = TRUE, fig.width=4, fig.height=4}
## Using rpart package:

library(rpart)
library(rattle)

model_fit <- rpart(formula = grade ~ ., method = "class", 
                   data = grades_data)

# Plot tree
fancyRpartPlot(model_fit)
```

<a name="two"></a>


&nbsp;

### <u>Example Two - Wine Quality Data</u>

&nbsp;

This second example deals with wine quality data. I load in the data from a website URL link.

&nbsp;

```{r}
# Example Two:

# Load packages:

library(party)
library(rpart)


url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv"

white_wine_data <- read.csv(url, header = TRUE, sep = ";")
```

&nbsp;

Previewing the data with the `head()` and `tail()` functions gives:

&nbsp;

```{r}
# Preview the data:
 
head(white_wine_data)
```

&nbsp;

You can examine the data a bit further with the `summary()` and `str()` functions in R.

&nbsp;

```{r}
# Check summary and data structure:
 
summary(white_wine_data)

str(white_wine_data)
```

&nbsp;

One decision tree I want to try out is predicting quality based on the wine's pH, alcohol content, sulphates and chlorides.

&nbsp;

```{r, echo = TRUE, fig.width=4, fig.height=4}
## Using rpart package:

library(rpart)
library(rpart.plot)

winemodel_fit <- rpart(formula = quality ~ pH + alcohol + sulphates + chlorides , method = "class", 
                   data = white_wine_data)

# Plot tree
#plot(winemodel_fit,  uniform = TRUE, main = "Asessing White Wine Quality")
# text(winemodel_fit, use.n = TRUE, all=TRUE, cex = .8)


rpart.plot(winemodel_fit) #Decision Tree Using Rpart.Plot
```

&nbsp;

This decision tree comes out nicely and you can easily read it from top to bottom. Note that the sample size is 4898. Numbers at the top of the nodes represent the predicted wine quality number. Quality scores are from 3 to 9. The percentages at the bottom represent the percentage of the sample size. As an example, the 63% in the second row left node is 63% of 4898. In this node, a quality score of 6 is predicted at .44 (or 44%). It appears that the predictions are using a likelihood approach where the quality score is chosen based on the highest percentage.


An alternate decision tree would include counts instead of percentages corresponding to the wine quality.

&nbsp;

```{r, echo = TRUE, fig.width=4, fig.height=4}
rpart.plot(winemodel_fit, type = 4, extra = 101) #Decision Tree Using Rpart.Plot
```

&nbsp;

From the rattle and RColorBrewer packages in R, there is an another alternate decision tree you can create. This tree has percentages and not counts in the nodes.

&nbsp;

```{r, echo = TRUE, fig.width=4, fig.height=4}
library(rattle)
library(RColorBrewer)

fancyRpartPlot(winemodel_fit)
```

&nbsp;

**A Full Decision Tree**

&nbsp;

The decision trees above were predicting wine quality with some of the other variables. This tree will predict wine quality with all of the other variables. This tree will have more nodes and branches.

&nbsp;

```{r, echo = TRUE, fig.width=5, fig.height=5} 
# Full Decision Tree:

winemodel_fit2 <- rpart(formula = quality ~ . , method = "class", 
                       data = white_wine_data)

rpart.plot(winemodel_fit2, type = 4, extra = 101) #Decision Tree Using Rpart.Plot


fancyRpartPlot(winemodel_fit2)

```

&nbsp;

These trees have added more detail by adding volatile.acidity and free.sulfur.dioxide into the decisions. I am not sure if these additional factors actually affect the quality and taste of the wine. It would be best to consult with wine experts to see if these results are valid. (Wine quality seems like a subjective measure as well.)

<a name="refs"></a>

&nbsp;

### <u>References</u>

&nbsp;

* https://www.tutorialspoint.com/r/r_decision_tree.htm
* https://www.r-bloggers.com/a-brief-tour-of-the-trees-and-forests/
* https://www.youtube.com/watch?v=JFJIQ0_2ijg
* http://trevorstephens.com/kaggle-titanic-tutorial/r-part-3-decision-trees/