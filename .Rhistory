train_data <- math_data_clean_std[train_ind, ]
train_success <- course_success[train_ind]
test_data <- math_data_clean_std[-train_ind, ]
test_success <- course_success[-train_ind]
## KNN:
library(class)
set.seed(101)
predicted_success <- knn(train_data, test_data, train_success, k = 1)
head(predicted_success)
mean(test_success != predicted_success)
predicted_success <- knn(train_data, test_data, train_success, k = 3)
head(predicted_success)
mean(test_success != predicted_success)
predicted_success <- knn(train_data, test_data, train_success, k = 10)
head(predicted_success)
mean(test_success != predicted_success)
predicted_success <- knn(train_data, test_data, train_success, k = 20)
head(predicted_success)
mean(test_success != predicted_success)
# For Loop apparoach for finding optimal k"
predicted_success = NULL
error_rate = NULL
for(i in 1:25){
set.seed(101)
predicted_success = knn(train_data, test_data,train_success, k=i)
error_rate[i] = mean(test_success != predicted_success)
}
# Elbow Method Plot With ggplot2
library(ggplot2)
error_df <- data.frame(k = 1:25, error_rate)
error_df
ggplot(error_df,aes(x= k ,y = error_rate)) +
geom_point() +
geom_line(lty="dotted",color='red')
## KNN:
library(class)
set.seed(101)
predicted_success <- knn(train_data, test_data, train_success, k = 1)
head(predicted_success)
mean(test_success != predicted_success)
predicted_success <- knn(train_data, test_data, train_success, k = 3)
head(predicted_success)
mean(test_success != predicted_success)
predicted_success <- knn(train_data, test_data, train_success, k = 10)
head(predicted_success)
mean(test_success != predicted_success)
# For Loop apparoach for finding optimal k"
predicted_success = NULL
error_rate = NULL
for(i in 1:15){
set.seed(101)
predicted_success = knn(train_data, test_data,train_success, k=i)
error_rate[i] = mean(test_success != predicted_success)
}
# Elbow Method Plot With ggplot2
library(ggplot2)
error_df <- data.frame(k = 1:25, error_rate)
error_df
ggplot(error_df,aes(x= k ,y = error_rate)) +
geom_point() +
geom_line(lty="dotted",color='red')
# Scaling The Dataset
# Have the CourseSuccess column in a separate variable:
course_success <- math_data_clean$CourseSuccess
# Standarize the dataset using "scale()" R function
math_data_clean_std <- scale(math_data_clean[, c(1:9, 11, 13:15)])
# Test set and training set:
# Reference: https://stackoverflow.com/questions/17200114/how-to-split-data-into-training-testing-sets-using-sample-function
## set the seed to make your partition reproducible
set.seed(251)
train_ind <- sample(seq_len(nrow(math_data_clean_std)), size = floor(0.60 * nrow(math_data_clean_std)))
train_data <- math_data_clean_std[train_ind, ]
train_success <- course_success[train_ind]
test_data <- math_data_clean_std[-train_ind, ]
test_success <- course_success[-train_ind]
## KNN:
library(class)
set.seed(101)
predicted_success <- knn(train_data, test_data, train_success, k = 1)
head(predicted_success)
mean(test_success != predicted_success)
predicted_success <- knn(train_data, test_data, train_success, k = 3)
head(predicted_success)
mean(test_success != predicted_success)
predicted_success <- knn(train_data, test_data, train_success, k = 10)
head(predicted_success)
mean(test_success != predicted_success)
# For Loop apparoach for finding optimal k"
predicted_success = NULL
error_rate = NULL
for(i in 1:15){
set.seed(101)
predicted_success = knn(train_data, test_data,train_success, k=i)
error_rate[i] = mean(test_success != predicted_success)
}
# Elbow Method Plot With ggplot2
library(ggplot2)
error_df <- data.frame(k = 1:15, error_rate)
error_df
ggplot(error_df,aes(x= k ,y = error_rate)) +
geom_point() +
geom_line(lty="dotted",color='red')
seq_len(nrow(math_data_clean_std)
)
train_ind
train_data <- math_data_clean_std[train_ind, ]
train_success <- course_success[train_ind]
train_success
train_data
test_data
test_success
?knn
head(predicted_success)
mean(test_success != predicted_success)
install.packages("ISLR")
library(ISLR)
# K-Nearest Neighbours Practice In R
# Reference: Udemy Course - R For Data Science & Machine Learning By Jose Portilla
# Book & Library Reference: Introduction To Statistical Learning With R
# Book Info Link:https://www-bcf.usc.edu/~gareth/ISL/
library(ISLR)
library(ggplot2)
# Load data into math_data variable:
default_data <- Default
# Head and tail of data:
head(default_data)
tail(default_data )
# Structure of data:
str(default_data)
sum(is.na(default_data))
any(is.na(default_data))
sum(is.na(default_data))
# K-Nearest Neighbours Practice In R
# Reference: Udemy Course - R For Data Science & Machine Learning By Jose Portilla
# Book & Library Reference: Introduction To Statistical Learning With R
# Book Info Link:https://www-bcf.usc.edu/~gareth/ISL/
library(ISLR)
library(ggplot2)
# Load data into math_data variable:
default_data <- Default
# Head and tail of data:
head(default_data)
tail(default_data )
# Structure of data:
str(default_data)
### Some data cleaning:
# Check if theres any missing data (NAs):
any(is.na(default_data))
sum(is.na(default_data))
### Standardize Variables With Scaling
# Standarize the dataset using "scale()" R function
default_data_std <- scale(default_data[, c(3:4)])
default_data_std
# Load data into math_data variable:
default_data <- Default
# Head and tail of data:
head(default_data)
tail(default_data )
# Structure of data:
str(default_data)
# K-Nearest Neighbours Practice In R
# Reference: Udemy Course - R For Data Science & Machine Learning By Jose Portilla
# Book & Library Reference: Introduction To Statistical Learning With R
# Book Info Link:https://www-bcf.usc.edu/~gareth/ISL/
library(ISLR)
library(ggplot2)
# Load data into math_data variable:
default_data <- Default
# Head and tail of data:
head(default_data)
tail(default_data )
# Structure of data:
str(default_data)
### Some data cleaning:
# Check if theres any missing data (NAs):
any(is.na(default_data))
sum(is.na(default_data))
### Standardize Variables With Scaling
# Standarize the dataset using "scale()" R function
default_data_std <- scale(default_data[, c(3:4)])
var(default_data_std[, 3])
var(default_data_std[:, 3])
default_data_std[, 3]
default_data_std[ 3]
scale(default_data[, c(3:4)])
default_data[, 1]
# K-Nearest Neighbours Practice In R
# Reference: Udemy Course - R For Data Science & Machine Learning By Jose Portilla
# Book & Library Reference: Introduction To Statistical Learning With R
# Book Info Link:https://www-bcf.usc.edu/~gareth/ISL/
library(ISLR)
library(ggplot2)
# Load data into math_data variable:
default_data <- Default
# Head and tail of data:
head(default_data)
tail(default_data )
# Structure of data:
str(default_data)
### Some data cleaning:
# Check if theres any missing data (NAs):
any(is.na(default_data))
sum(is.na(default_data))
### Standardize Variables With Scaling
# Standarize the dataset using "scale()" R function
default_data_std <- scale(default_data[, c(3:4)])
defaults_outcome <- default_data[, 1]
# Test set and training set:
# Reference: https://stackoverflow.com/questions/17200114/how-to-split-data-into-training-testing-sets-using-sample-function
## set the seed to make your partition reproducible
set.seed(311)
train_ind <- sample(seq_len(nrow(default_data_std)), size = floor(0.60 * nrow(math_data_clean_std)))
train_data <- default_data_std[train_ind, ]
train_success <- defaults_outcome[train_ind]
test_data <- default_data_std[-train_ind, ]
test_success <- defaults_outcome[-train_ind]
## KNN:
# Predict if there's a deafult based on balance and income
# (not factoring in student)
library(class)
set.seed(101)
predicted_success <- knn(train_data, test_data, train_success, k = 1)
head(predicted_success)
mean(test_success != predicted_success)
predicted_success <- knn(train_data, test_data, train_success, k = 3)
head(predicted_success)
mean(test_success != predicted_success)
predicted_success <- knn(train_data, test_data, train_success, k = 10)
head(predicted_success)
mean(test_success != predicted_success)
# For Loop apparoach for finding optimal k"
predicted_success = NULL
error_rate = NULL
for(i in 1:15){
set.seed(101)
predicted_success = knn(train_data, test_data,train_success, k=i)
error_rate[i] = mean(test_success != predicted_success)
}
# Elbow Method Plot With ggplot2
library(ggplot2)
error_df <- data.frame(k = 1:15, error_rate)
error_df
ggplot(error_df,aes(x= k ,y = error_rate)) +
geom_point() +
geom_line(lty="dotted",color='red')
# K-Nearest Neighbours Practice In R
# Reference: Udemy Course - R For Data Science & Machine Learning By Jose Portilla
# Book & Library Reference: Introduction To Statistical Learning With R
# Book Info Link:https://www-bcf.usc.edu/~gareth/ISL/
library(ISLR)
library(ggplot2)
# Load data into math_data variable:
default_data <- Default
# Head and tail of data:
head(default_data)
tail(default_data )
# Structure of data:
str(default_data)
### Some data cleaning:
# Check if theres any missing data (NAs):
any(is.na(default_data))
sum(is.na(default_data))
### Standardize Variables With Scaling
# Standarize the dataset using "scale()" R function
default_data_std <- scale(default_data[, c(3:4)])
defaults_outcome <- default_data[, 1]
# Test set and training set:
# Reference: https://stackoverflow.com/questions/17200114/how-to-split-data-into-training-testing-sets-using-sample-function
## set the seed to make your partition reproducible
set.seed(311)
train_ind <- sample(seq_len(nrow(default_data_std)), size = floor(0.60 * nrow(math_data_clean_std)))
train_data <- default_data_std[train_ind, ]
train_defaults <- defaults_outcome[train_ind]
test_data <- default_data_std[-train_ind, ]
test_defaults <- defaults_outcome[-train_ind]
## KNN:
# Predict if there's a deafult based on balance and income
# (not factoring in student)
library(class)
set.seed(101)
predicted_defaults <- knn(train_data, test_data, train_defaults, k = 1)
head(predicted_defaults)
mean(test_defaults != predicted_defaults)
# Test set and training set:
# Reference: https://stackoverflow.com/questions/17200114/how-to-split-data-into-training-testing-sets-using-sample-function
## set the seed to make your partition reproducible
set.seed(311)
train_ind <- sample(seq_len(nrow(default_data_std)), size = floor(0.60 * nrow(default_data_std)))
train_data <- default_data_std[train_ind, ]
train_defaults <- defaults_outcome[train_ind]
test_data <- default_data_std[-train_ind, ]
test_defaults <- defaults_outcome[-train_ind]
## KNN:
# Predict if there's a deafult based on balance and income
# (not factoring in student)
library(class)
set.seed(101)
predicted_defaults <- knn(train_data, test_data, train_defaults, k = 1)
head(predicted_defaults)
mean(test_defaults != predicted_defaults)
predicted_defaults <- knn(train_data, test_data, train_defaults, k = 3)
head(predicted_defaults)
mean(test_defaults != predicted_defaults)
predicted_defaults <- knn(train_data, test_data, train_defaults, k = 5)
head(predicted_defaults)
mean(test_defaults != predicted_defaults)
# For Loop apparoach for finding optimal k"
predicted_defaults = NULL
error_rate = NULL
for(i in 1:15){
set.seed(101)
predicted_defaults <- knn(train_data, test_data, train_defaults, k = i)
error_rate[i] = mean(test_defaults != predicted_defaults)
}
# Elbow Method Plot With ggplot2
library(ggplot2)
error_df <- data.frame(k = 1:15, error_rate)
error_df
ggplot(error_df,aes(x= k ,y = error_rate)) +
geom_point() +
geom_line(lty="dotted",color='red')
ggplot(error_df,aes(x= k ,y = error_rate)) +
geom_point() +
geom_line(lty="dotted",color='red')
labs(x = "\n Number Of Neighbours (k)", y = "Error \n",
title = "Selecting Optimal Value For k \n") +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour = "darkgreen", size = 12),
axis.title.y = element_text(face="bold", colour = "darkgreen", size = 12),
legend.title = element_text(face="bold", size = 10))
ggplot(error_df, aes(x = k ,y = error_rate)) +
geom_point() +
geom_line(lty="dotted",color='red')
labs(x = "\n Number Of Neighbours (k)", y = "Error \n",
title = "Selecting Optimal Value For k \n") +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour = "darkgreen", size = 12),
axis.title.y = element_text(face="bold", colour = "darkgreen", size = 12),
legend.title = element_text(face="bold", size = 10))
# K-Nearest Neighbours Practice In R
# Reference: Udemy Course - R For Data Science & Machine Learning By Jose Portilla
# Book & Library Reference: Introduction To Statistical Learning With R
# Book Info Link:https://www-bcf.usc.edu/~gareth/ISL/
library(ISLR)
library(ggplot2)
# Load data into math_data variable:
default_data <- Default
# Head and tail of data:
head(default_data)
tail(default_data )
# Structure of data:
str(default_data)
### Some data cleaning:
# Check if theres any missing data (NAs):
any(is.na(default_data))
sum(is.na(default_data))
### Standardize Variables With Scaling
# Standarize the dataset using "scale()" R function
default_data_std <- scale(default_data[, c(3:4)])
defaults_outcome <- default_data[, 1]
# Test set and training set:
# Reference: https://stackoverflow.com/questions/17200114/how-to-split-data-into-training-testing-sets-using-sample-function
## set the seed to make your partition reproducible
set.seed(311)
train_ind <- sample(seq_len(nrow(default_data_std)), size = floor(0.60 * nrow(default_data_std)))
train_data <- default_data_std[train_ind, ]
train_defaults <- defaults_outcome[train_ind]
test_data <- default_data_std[-train_ind, ]
test_defaults <- defaults_outcome[-train_ind]
## KNN:
# Predict if there's a deafult based on balance and income
# (not factoring in student)
library(class)
set.seed(101)
predicted_defaults <- knn(train_data, test_data, train_defaults, k = 1)
head(predicted_defaults)
mean(test_defaults != predicted_defaults)
# KNN With k = 3:
predicted_defaults <- knn(train_data, test_data, train_defaults, k = 3)
head(predicted_defaults)
mean(test_defaults != predicted_defaults)
# KNN With k = 5:
predicted_defaults <- knn(train_data, test_data, train_defaults, k = 5)
head(predicted_defaults)
mean(test_defaults != predicted_defaults)
# For Loop apparoach for finding optimal k"
predicted_defaults = NULL
error_rate = NULL
for(i in 1:15){
set.seed(101)
predicted_defaults <- knn(train_data, test_data, train_defaults, k = i)
error_rate[i] = mean(test_defaults != predicted_defaults)
}
# Elbow Method Plot With ggplot2
library(ggplot2)
error_df <- data.frame(k = 1:15, error_rate)
error_df
ggplot(error_df, aes(x = k ,y = error_rate)) +
geom_point() +
geom_line(lty="dotted",color='red')
labs(x = "\n Number Of Neighbours (k)", y = "Error \n",
title = "Selecting Optimal Value For k \n") +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour = "darkgreen", size = 12),
axis.title.y = element_text(face="bold", colour = "darkgreen", size = 12),
legend.title = element_text(face="bold", size = 10))
ggplot(error_df, aes(x = k ,y = error_rate)) +
geom_point() +
geom_line(lty="dotted",color='red') +
labs(x = "\n Number Of Neighbours (k)", y = "Error \n",
title = "Selecting Optimal Value For k \n") +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour = "darkgreen", size = 12),
axis.title.y = element_text(face="bold", colour = "darkgreen", size = 12),
legend.title = element_text(face="bold", size = 10))
# Elbow plot with ggplot2
ggplot(error_df, aes(x = k ,y = error_rate)) +
geom_point() +
geom_line(lty="dotted",color='red') +
labs(x = "\n Number Of Neighbours (k)", y = "Error \n",
title = "K-Nearest Neighbours \n Selecting Optimal Value For k \n") +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour = "darkgreen", size = 12),
axis.title.y = element_text(face="bold", colour = "darkgreen", size = 12),
legend.title = element_text(face="bold", size = 10))
# For Loop apparoach for finding optimal k"
predicted_defaults = NULL
error_rate = NULL
for(i in 1:25){
set.seed(101)
predicted_defaults <- knn(train_data, test_data, train_defaults, k = i)
error_rate[i] = mean(test_defaults != predicted_defaults)
}
# Elbow Method Plot With ggplot2
library(ggplot2)
error_df <- data.frame(k = 1:25, error_rate)
error_df
# Elbow plot with ggplot2
ggplot(error_df, aes(x = k ,y = error_rate)) +
geom_point() +
geom_line(lty="dotted",color='red') +
labs(x = "\n Number Of Neighbours (k)", y = "Error \n",
title = "K-Nearest Neighbours \n Selecting Optimal Value For k \n") +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour = "darkgreen", size = 12),
axis.title.y = element_text(face="bold", colour = "darkgreen", size = 12),
legend.title = element_text(face="bold", size = 10))
# For Loop apparoach for finding optimal k"
predicted_defaults = NULL
error_rate = NULL
for(i in 1:20){
set.seed(101)
predicted_defaults <- knn(train_data, test_data, train_defaults, k = i)
error_rate[i] = mean(test_defaults != predicted_defaults)
}
# Elbow Method Plot With ggplot2
library(ggplot2)
error_df <- data.frame(k = 1:20, error_rate)
error_df
# Elbow plot with ggplot2
ggplot(error_df, aes(x = k ,y = error_rate)) +
geom_point() +
geom_line(lty="dotted",color='red') +
labs(x = "\n Number Of Neighbours (k)", y = "Error \n",
title = "K-Nearest Neighbours \n Selecting Optimal Value For k \n") +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour = "darkgreen", size = 12),
axis.title.y = element_text(face="bold", colour = "darkgreen", size = 12),
legend.title = element_text(face="bold", size = 10))
# For Loop approach for finding optimal k"
predicted_defaults = NULL
error_rate = NULL
for(i in 1:20){
set.seed(101)
predicted_defaults <- knn(train_data, test_data, train_defaults, k = i)
error_rate[i] = mean(test_defaults != predicted_defaults)
}
# Elbow Method Plot With ggplot2
library(ggplot2)
error_df <- data.frame(k = 1:20, error_rate)
error_df
# Elbow plot with ggplot2
ggplot(error_df, aes(x = k ,y = error_rate)) +
geom_point() +
geom_line(lty="dotted",color='red') +
labs(x = "\n Number Of Neighbours (k)", y = "Error \n",
title = "K-Nearest Neighbours \n Selecting Optimal Value For k \n") +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour = "darkgreen", size = 12),
axis.title.y = element_text(face="bold", colour = "darkgreen", size = 12),
legend.title = element_text(face="bold", size = 10))
tail(default_data)
# Initial scatterplot Of Balance vs Income:
ggplot(default_data, aes(x = balance ,y = income)) +
geom_point() +
labs(x = "\n Balance", y = "Income \n",
title = "Balance Vs Income \n") +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour = "darkgreen", size = 12),
axis.title.y = element_text(face="bold", colour = "darkgreen", size = 12),
legend.title = element_text(face="bold", size = 10))
# Initial scatterplot Of Balance vs Income:
ggplot(default_data, aes(x = income ,y = balance)) +
geom_point() +
labs(x = "\n Balance", y = "Income \n",
title = "Balance Vs Income \n") +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour = "darkgreen", size = 12),
axis.title.y = element_text(face="bold", colour = "darkgreen", size = 12),
legend.title = element_text(face="bold", size = 10))
setwd("~/dkmathstats_site")
rmarkdown::render_site()
