# Load data into math_data variable:
default_data <- Default
# Head and tail of data:
head(default_data)
tail(default_data )
# Structure of data:
str(default_data)
### Some data cleaning:
# Check if theres any missing data (NAs):
any(is.na(default_data))
sum(is.na(default_data))
### Standardize Variables With Scaling
# Standarize the dataset using "scale()" R function
default_data_std <- scale(default_data[, c(3:4)])
var(default_data_std[, 3])
var(default_data_std[:, 3])
default_data_std[, 3]
default_data_std[ 3]
scale(default_data[, c(3:4)])
default_data[, 1]
# K-Nearest Neighbours Practice In R
# Reference: Udemy Course - R For Data Science & Machine Learning By Jose Portilla
# Book & Library Reference: Introduction To Statistical Learning With R
# Book Info Link:https://www-bcf.usc.edu/~gareth/ISL/
library(ISLR)
library(ggplot2)
# Load data into math_data variable:
default_data <- Default
# Head and tail of data:
head(default_data)
tail(default_data )
# Structure of data:
str(default_data)
### Some data cleaning:
# Check if theres any missing data (NAs):
any(is.na(default_data))
sum(is.na(default_data))
### Standardize Variables With Scaling
# Standarize the dataset using "scale()" R function
default_data_std <- scale(default_data[, c(3:4)])
defaults_outcome <- default_data[, 1]
# Test set and training set:
# Reference: https://stackoverflow.com/questions/17200114/how-to-split-data-into-training-testing-sets-using-sample-function
## set the seed to make your partition reproducible
set.seed(311)
train_ind <- sample(seq_len(nrow(default_data_std)), size = floor(0.60 * nrow(math_data_clean_std)))
train_data <- default_data_std[train_ind, ]
train_success <- defaults_outcome[train_ind]
test_data <- default_data_std[-train_ind, ]
test_success <- defaults_outcome[-train_ind]
## KNN:
# Predict if there's a deafult based on balance and income
# (not factoring in student)
library(class)
set.seed(101)
predicted_success <- knn(train_data, test_data, train_success, k = 1)
head(predicted_success)
mean(test_success != predicted_success)
predicted_success <- knn(train_data, test_data, train_success, k = 3)
head(predicted_success)
mean(test_success != predicted_success)
predicted_success <- knn(train_data, test_data, train_success, k = 10)
head(predicted_success)
mean(test_success != predicted_success)
# For Loop apparoach for finding optimal k"
predicted_success = NULL
error_rate = NULL
for(i in 1:15){
set.seed(101)
predicted_success = knn(train_data, test_data,train_success, k=i)
error_rate[i] = mean(test_success != predicted_success)
}
# Elbow Method Plot With ggplot2
library(ggplot2)
error_df <- data.frame(k = 1:15, error_rate)
error_df
ggplot(error_df,aes(x= k ,y = error_rate)) +
geom_point() +
geom_line(lty="dotted",color='red')
# K-Nearest Neighbours Practice In R
# Reference: Udemy Course - R For Data Science & Machine Learning By Jose Portilla
# Book & Library Reference: Introduction To Statistical Learning With R
# Book Info Link:https://www-bcf.usc.edu/~gareth/ISL/
library(ISLR)
library(ggplot2)
# Load data into math_data variable:
default_data <- Default
# Head and tail of data:
head(default_data)
tail(default_data )
# Structure of data:
str(default_data)
### Some data cleaning:
# Check if theres any missing data (NAs):
any(is.na(default_data))
sum(is.na(default_data))
### Standardize Variables With Scaling
# Standarize the dataset using "scale()" R function
default_data_std <- scale(default_data[, c(3:4)])
defaults_outcome <- default_data[, 1]
# Test set and training set:
# Reference: https://stackoverflow.com/questions/17200114/how-to-split-data-into-training-testing-sets-using-sample-function
## set the seed to make your partition reproducible
set.seed(311)
train_ind <- sample(seq_len(nrow(default_data_std)), size = floor(0.60 * nrow(math_data_clean_std)))
train_data <- default_data_std[train_ind, ]
train_defaults <- defaults_outcome[train_ind]
test_data <- default_data_std[-train_ind, ]
test_defaults <- defaults_outcome[-train_ind]
## KNN:
# Predict if there's a deafult based on balance and income
# (not factoring in student)
library(class)
set.seed(101)
predicted_defaults <- knn(train_data, test_data, train_defaults, k = 1)
head(predicted_defaults)
mean(test_defaults != predicted_defaults)
# Test set and training set:
# Reference: https://stackoverflow.com/questions/17200114/how-to-split-data-into-training-testing-sets-using-sample-function
## set the seed to make your partition reproducible
set.seed(311)
train_ind <- sample(seq_len(nrow(default_data_std)), size = floor(0.60 * nrow(default_data_std)))
train_data <- default_data_std[train_ind, ]
train_defaults <- defaults_outcome[train_ind]
test_data <- default_data_std[-train_ind, ]
test_defaults <- defaults_outcome[-train_ind]
## KNN:
# Predict if there's a deafult based on balance and income
# (not factoring in student)
library(class)
set.seed(101)
predicted_defaults <- knn(train_data, test_data, train_defaults, k = 1)
head(predicted_defaults)
mean(test_defaults != predicted_defaults)
predicted_defaults <- knn(train_data, test_data, train_defaults, k = 3)
head(predicted_defaults)
mean(test_defaults != predicted_defaults)
predicted_defaults <- knn(train_data, test_data, train_defaults, k = 5)
head(predicted_defaults)
mean(test_defaults != predicted_defaults)
# For Loop apparoach for finding optimal k"
predicted_defaults = NULL
error_rate = NULL
for(i in 1:15){
set.seed(101)
predicted_defaults <- knn(train_data, test_data, train_defaults, k = i)
error_rate[i] = mean(test_defaults != predicted_defaults)
}
# Elbow Method Plot With ggplot2
library(ggplot2)
error_df <- data.frame(k = 1:15, error_rate)
error_df
ggplot(error_df,aes(x= k ,y = error_rate)) +
geom_point() +
geom_line(lty="dotted",color='red')
ggplot(error_df,aes(x= k ,y = error_rate)) +
geom_point() +
geom_line(lty="dotted",color='red')
labs(x = "\n Number Of Neighbours (k)", y = "Error \n",
title = "Selecting Optimal Value For k \n") +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour = "darkgreen", size = 12),
axis.title.y = element_text(face="bold", colour = "darkgreen", size = 12),
legend.title = element_text(face="bold", size = 10))
ggplot(error_df, aes(x = k ,y = error_rate)) +
geom_point() +
geom_line(lty="dotted",color='red')
labs(x = "\n Number Of Neighbours (k)", y = "Error \n",
title = "Selecting Optimal Value For k \n") +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour = "darkgreen", size = 12),
axis.title.y = element_text(face="bold", colour = "darkgreen", size = 12),
legend.title = element_text(face="bold", size = 10))
# K-Nearest Neighbours Practice In R
# Reference: Udemy Course - R For Data Science & Machine Learning By Jose Portilla
# Book & Library Reference: Introduction To Statistical Learning With R
# Book Info Link:https://www-bcf.usc.edu/~gareth/ISL/
library(ISLR)
library(ggplot2)
# Load data into math_data variable:
default_data <- Default
# Head and tail of data:
head(default_data)
tail(default_data )
# Structure of data:
str(default_data)
### Some data cleaning:
# Check if theres any missing data (NAs):
any(is.na(default_data))
sum(is.na(default_data))
### Standardize Variables With Scaling
# Standarize the dataset using "scale()" R function
default_data_std <- scale(default_data[, c(3:4)])
defaults_outcome <- default_data[, 1]
# Test set and training set:
# Reference: https://stackoverflow.com/questions/17200114/how-to-split-data-into-training-testing-sets-using-sample-function
## set the seed to make your partition reproducible
set.seed(311)
train_ind <- sample(seq_len(nrow(default_data_std)), size = floor(0.60 * nrow(default_data_std)))
train_data <- default_data_std[train_ind, ]
train_defaults <- defaults_outcome[train_ind]
test_data <- default_data_std[-train_ind, ]
test_defaults <- defaults_outcome[-train_ind]
## KNN:
# Predict if there's a deafult based on balance and income
# (not factoring in student)
library(class)
set.seed(101)
predicted_defaults <- knn(train_data, test_data, train_defaults, k = 1)
head(predicted_defaults)
mean(test_defaults != predicted_defaults)
# KNN With k = 3:
predicted_defaults <- knn(train_data, test_data, train_defaults, k = 3)
head(predicted_defaults)
mean(test_defaults != predicted_defaults)
# KNN With k = 5:
predicted_defaults <- knn(train_data, test_data, train_defaults, k = 5)
head(predicted_defaults)
mean(test_defaults != predicted_defaults)
# For Loop apparoach for finding optimal k"
predicted_defaults = NULL
error_rate = NULL
for(i in 1:15){
set.seed(101)
predicted_defaults <- knn(train_data, test_data, train_defaults, k = i)
error_rate[i] = mean(test_defaults != predicted_defaults)
}
# Elbow Method Plot With ggplot2
library(ggplot2)
error_df <- data.frame(k = 1:15, error_rate)
error_df
ggplot(error_df, aes(x = k ,y = error_rate)) +
geom_point() +
geom_line(lty="dotted",color='red')
labs(x = "\n Number Of Neighbours (k)", y = "Error \n",
title = "Selecting Optimal Value For k \n") +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour = "darkgreen", size = 12),
axis.title.y = element_text(face="bold", colour = "darkgreen", size = 12),
legend.title = element_text(face="bold", size = 10))
ggplot(error_df, aes(x = k ,y = error_rate)) +
geom_point() +
geom_line(lty="dotted",color='red') +
labs(x = "\n Number Of Neighbours (k)", y = "Error \n",
title = "Selecting Optimal Value For k \n") +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour = "darkgreen", size = 12),
axis.title.y = element_text(face="bold", colour = "darkgreen", size = 12),
legend.title = element_text(face="bold", size = 10))
# Elbow plot with ggplot2
ggplot(error_df, aes(x = k ,y = error_rate)) +
geom_point() +
geom_line(lty="dotted",color='red') +
labs(x = "\n Number Of Neighbours (k)", y = "Error \n",
title = "K-Nearest Neighbours \n Selecting Optimal Value For k \n") +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour = "darkgreen", size = 12),
axis.title.y = element_text(face="bold", colour = "darkgreen", size = 12),
legend.title = element_text(face="bold", size = 10))
# For Loop apparoach for finding optimal k"
predicted_defaults = NULL
error_rate = NULL
for(i in 1:25){
set.seed(101)
predicted_defaults <- knn(train_data, test_data, train_defaults, k = i)
error_rate[i] = mean(test_defaults != predicted_defaults)
}
# Elbow Method Plot With ggplot2
library(ggplot2)
error_df <- data.frame(k = 1:25, error_rate)
error_df
# Elbow plot with ggplot2
ggplot(error_df, aes(x = k ,y = error_rate)) +
geom_point() +
geom_line(lty="dotted",color='red') +
labs(x = "\n Number Of Neighbours (k)", y = "Error \n",
title = "K-Nearest Neighbours \n Selecting Optimal Value For k \n") +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour = "darkgreen", size = 12),
axis.title.y = element_text(face="bold", colour = "darkgreen", size = 12),
legend.title = element_text(face="bold", size = 10))
# For Loop apparoach for finding optimal k"
predicted_defaults = NULL
error_rate = NULL
for(i in 1:20){
set.seed(101)
predicted_defaults <- knn(train_data, test_data, train_defaults, k = i)
error_rate[i] = mean(test_defaults != predicted_defaults)
}
# Elbow Method Plot With ggplot2
library(ggplot2)
error_df <- data.frame(k = 1:20, error_rate)
error_df
# Elbow plot with ggplot2
ggplot(error_df, aes(x = k ,y = error_rate)) +
geom_point() +
geom_line(lty="dotted",color='red') +
labs(x = "\n Number Of Neighbours (k)", y = "Error \n",
title = "K-Nearest Neighbours \n Selecting Optimal Value For k \n") +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour = "darkgreen", size = 12),
axis.title.y = element_text(face="bold", colour = "darkgreen", size = 12),
legend.title = element_text(face="bold", size = 10))
# For Loop approach for finding optimal k"
predicted_defaults = NULL
error_rate = NULL
for(i in 1:20){
set.seed(101)
predicted_defaults <- knn(train_data, test_data, train_defaults, k = i)
error_rate[i] = mean(test_defaults != predicted_defaults)
}
# Elbow Method Plot With ggplot2
library(ggplot2)
error_df <- data.frame(k = 1:20, error_rate)
error_df
# Elbow plot with ggplot2
ggplot(error_df, aes(x = k ,y = error_rate)) +
geom_point() +
geom_line(lty="dotted",color='red') +
labs(x = "\n Number Of Neighbours (k)", y = "Error \n",
title = "K-Nearest Neighbours \n Selecting Optimal Value For k \n") +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour = "darkgreen", size = 12),
axis.title.y = element_text(face="bold", colour = "darkgreen", size = 12),
legend.title = element_text(face="bold", size = 10))
tail(default_data)
# Initial scatterplot Of Balance vs Income:
ggplot(default_data, aes(x = balance ,y = income)) +
geom_point() +
labs(x = "\n Balance", y = "Income \n",
title = "Balance Vs Income \n") +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour = "darkgreen", size = 12),
axis.title.y = element_text(face="bold", colour = "darkgreen", size = 12),
legend.title = element_text(face="bold", size = 10))
# Initial scatterplot Of Balance vs Income:
ggplot(default_data, aes(x = income ,y = balance)) +
geom_point() +
labs(x = "\n Balance", y = "Income \n",
title = "Balance Vs Income \n") +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour = "darkgreen", size = 12),
axis.title.y = element_text(face="bold", colour = "darkgreen", size = 12),
legend.title = element_text(face="bold", size = 10))
# Logistic Regression In R
# Reference: Udemy Course - R For Data Science & Machine Learning By Jose Portilla
# Book & Library Reference: Introduction To Statistical Learning With R
# Book Info Link:https://www-bcf.usc.edu/~gareth/ISL/
library(ISLR)
library(ggplot2)
default_data <- Default
# Preview the data:
head(default_data)
tail(default_data)
str(default_data)
# Check default column on counts
table(default_data$default)
# Initial Barplot:
ggplot(default_data, aes(x = default)) +
geom_bar() +
labs(x = "\n Tape Speed", y = "Laser Power\n",
title = "Strength Of Thermoplastic Composite \n Experiment Results \n",
fill = "Composite \n Strength \n") +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour="darkgreen", size = 12),
axis.title.y = element_text(face="bold", colour="darkgreen", size = 12),
legend.title = element_text(face="bold", colour="brown", size = 10))
# Initial Barplot:
ggplot(default_data, aes(x = default)) +
geom_bar() +
labs(x = "\n Tape Speed", y = "Laser Power\n",
title = "Strength Of Thermoplastic Composite \n Experiment Results \n",
fill = "Composite \n Strength \n") +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour="darkgreen", size = 12),
axis.title.y = element_text(face="bold", colour="darkgreen", size = 12),
legend.title = element_text(face="bold", colour="brown", size = 10)) +
geom_text(aes(x = default, y = Count, label = Count), color = "white",
fontface = "bold", size = 5)
# Initial Barplot:
ggplot(default_data, aes(x = default)) +
geom_bar() +
labs(x = "\n Tape Speed", y = "Laser Power\n",
title = "Strength Of Thermoplastic Composite \n Experiment Results \n",
fill = "Composite \n Strength \n") +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour="darkgreen", size = 12),
axis.title.y = element_text(face="bold", colour="darkgreen", size = 12),
legend.title = element_text(face="bold", colour="brown", size = 10)) +
geom_text(aes(x = default, label = Count), color = "white",
fontface = "bold", size = 5)
# Initial Barplot (No Labels):
ggplot(default_data, aes(x = default)) +
geom_bar() +
labs(x = "\n Tape Speed", y = "Laser Power\n",
title = "Strength Of Thermoplastic Composite \n Experiment Results \n",
fill = "Composite \n Strength \n") +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour="darkgreen", size = 12),
axis.title.y = element_text(face="bold", colour="darkgreen", size = 12),
legend.title = element_text(face="bold", colour="brown", size = 10))
# Initial Barplot (No Labels):
ggplot(default_data, aes(x = default)) +
geom_bar() +
labs(x = "\n Credit Default Status", y = "Count\n",
title = "Credit Default Status Of Customers \n") +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour="darkgreen", size = 12),
axis.title.y = element_text(face="bold", colour="darkgreen", size = 12),
legend.title = element_text(face="bold", colour="brown", size = 10))
# Running logistic regression model:
log_model <- glm(formula = default ~ . , family = binomial(link='logit'), data = default_data)
summary(log_model)
install.packages("caTools")
# Running logistic regression model:
log_model <- glm(formula = default ~ . , family = binomial(link='logit'), data = default_data)
# Run summary of logistic model:
summary(log_model)
# Using test cases for predictions:
library(caTools)
set.seed(101)
split = sample.split(default_data$default, SplitRatio = 0.70)
train_data = subset(default_data, split == TRUE)
test_data = subset(default_data, split == FALSE)
library(caTools)
set.seed(101)
split = sample.split(default_data$default, SplitRatio = 0.70)
train_data = subset(default_data, split == TRUE)
test_data = subset(default_data, split == FALSE)
# Running the logistic regression model again:
log_model2 <- glm(formula = default ~ . , family = binomial(link='logit'), data = train_data)
summary(log_model2)
summary(log_model2)
log_model2 <- glm(formula = default ~ . , family = binomial(link='logit'), data = train_data)
summary(log_model2)
fitted_probs <- predict(log_model2, newdata = test_data, type = 'response')
table(test_data$default, fitted_probs > 0.5)
misClasificError <- mean(fitted_probs != test_data$default)
print(paste('Accuracy',1-misClasificError))
fitted.results <- ifelse(fitted.probabilities > 0.5,1,0)
misClasificError <- mean(fitted.results!= test_data$default)
print(paste('Accuracy',1-misClasificError))
# Logistic Regression In R
# Reference: Udemy Course - R For Data Science & Machine Learning By Jose Portilla
# Book & Library Reference: Introduction To Statistical Learning With R
# Book Info Link:https://www-bcf.usc.edu/~gareth/ISL/
library(ISLR)
library(ggplot2)
default_data <- Default
# Preview the data:
head(default_data)
tail(default_data)
str(default_data)
# Check default column on counts
table(default_data$default)
# Initial Barplot (No Labels):
ggplot(default_data, aes(x = default)) +
geom_bar() +
labs(x = "\n Credit Default Status", y = "Count\n",
title = "Credit Default Status Of Customers \n") +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour="darkgreen", size = 12),
axis.title.y = element_text(face="bold", colour="darkgreen", size = 12),
legend.title = element_text(face="bold", colour="brown", size = 10))
# For logistic regression purposes: change No to 0 and Yes to 1 for default column.
# Combine never-worked and without pay groups into unemployed
default_change <- function(def_status){
if (def_status== 'Yes'){
return(1)
}else{
return(0)
}
}
# Use sapply on type_employer column:
default_data$default <- sapply(default_data$default, default_change)
# Check with table() and str() functions.
table(default_data$default)
str(default_data)
# Running logistic regression model:
log_model <- glm(formula = default ~ . , family = binomial(link='logit'), data = default_data)
# Run summary of logistic model:
summary(log_model)
# Using test cases for predictions:
library(caTools)
set.seed(101)
split = sample.split(default_data$default, SplitRatio = 0.70)
train_data = subset(default_data, split == TRUE)
test_data = subset(default_data, split == FALSE)
# Running the logistic regression model again:
log_model2 <- glm(formula = default ~ . , family = binomial(link='logit'), data = train_data)
summary(log_model2)
# Model Diagnostics
fitted_probs <- predict(log_model2, newdata = test_data, type = 'response')
fitted_results <- ifelse(fitted_probs > 0.5,1,0)
misClassError <- mean(fitted_results != test_data$default)
print(paste('Accuracy',1 - misClassError))
fitted_probs
table(test_data$default, fitted_probs > 0.5)
install.packages("caTools")
0.20 * 78 + 0.3 * 88 + 0.5 * 67
0.6^2
0.36*0.4
(1 - 0.978)^2
0.978 * (1 - 0.978)^2
0.4^3
1 - 0.4^3
rmarkdown::render_site
rmarkdown::render_site
rmarkdown::render_site()
rmarkdown::render_site()
setwd("~/dkmathstats_site")
rmarkdown::render_site()
