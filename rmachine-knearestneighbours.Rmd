---
title: "K Nearest Neighbours In R"
output: html_document
---

&nbsp;

Hi. This page featured the K Nearest Neighbours algorithm in machine/statistical learning. The R programming language is used here.

&nbsp;

### <u>Table Of Contents</u>

&nbsp;

* <a href="#refs">References</a>

* <a href="#data">Credit Card Default Data</a>

* <a href="#scaling">Standardize Variables With Scaling</a>

* <a href="#traintest">Building A Training Set And A Test Set</a>

* <a href="#knn">K-Nearest Neighbours In R</a>

* <a href="#optimal">Finding The Optimal Number Of K Neighbours</a>



&nbsp;

<a name="refs"></a>

### <u>References</u>

&nbsp;

* Udemy Course - R For Data Science & Machine Learning By Jose Portilla

* [Introduction To Statistical Learning With Applications In R Book](https://www-bcf.usc.edu/~gareth/ISL/)

* R Graphics Cookbook By Winston Chang

&nbsp;

<a name="data"></a>

### <u>Credit Card Default Data</u>

&nbsp;

The Introduction To Statistical Learning With Applications In R Book comes with a collection of datasets that are available in R. These datasets are in the `ISLR` library. To install the package use `install.packages("ISLR")`.

The credit card default data from `ISLR` is from Default. I take a brief look of the dataset with the use of `head()`, `tail()` and `str()` functions. 

&nbsp;

```{r, echo = TRUE, fig.width=4, fig.height=4} 
# K-Nearest Neighbours Practice In R
# Reference: Udemy Course - R For Data Science & Machine Learning By Jose Portilla

# Book & Library Reference: Introduction To Statistical Learning With R
# Book Info Link: https://www-bcf.usc.edu/~gareth/ISL/ 

library(ISLR)
library(ggplot2)

# Load data into math_data variable:

default_data <- Default

# Head and tail of data:

head(default_data)

tail(default_data)

# Structure of data:

str(default_data)


```

&nbsp;

It is a good idea to check if there are any missing values (NAs).

```{r}
### Some data cleaning:

# Check if theres any missing data (NAs):

any(is.na(default_data))

sum(is.na(default_data))

```

You can see what the data looks like with a simple scatterplot with R's `ggplot2` graphics. 


&nbsp;

```{r, echo = TRUE, fig.width=4, fig.height=4} 
# Initial scatterplot Of Balance vs Income:

ggplot(default_data, aes(x = income ,y = balance)) + 
  geom_point() + 
  labs(x = "\n Balance", y = "Income \n", 
       title = "Balance Vs Income \n") + 
  theme(plot.title = element_text(hjust = 0.5), 
        axis.title.x = element_text(face="bold", colour = "darkgreen", size = 12),
        axis.title.y = element_text(face="bold", colour = "darkgreen", size = 12),
        legend.title = element_text(face="bold", size = 10))
```

&nbsp;

The scatterplot contains a lot of points as there are 10000 observations in the dataset. Notice how there are observations with zero income but with non-negative balances.


<a name="scaling"></a>

&nbsp;

## Standardize Variables With Scaling

&nbsp;

Before running the K-Nearest Neighbours Algorithm, it is important to scale the numeric columns such that the numeric columns have a variance of 1. 

```{r}
### Standardize Variables With Scaling

# Standarize the dataset using "scale()" R function

default_data_std <- scale(default_data[, c(3:4)])

defaults_outcome <- default_data[, 1]

# Variance-Covariance Matrix:

var(default_data_std)
```


<a name="traintest"></a>

&nbsp;

## Building A Training Set And A Test Set

This next part deals with building a test set and a training set from the credit card default data. I am using 60% of the data for building the training set and the other 40% for the test set. Predictions from the K-Nearest Neighbours Algorithm will be compared to the real outcomes from the test set. In R, the `sample()` function is used for building the training set.

&nbsp;

```{r}
# Test set and training set:

# Reference: https://stackoverflow.com/questions/17200114/how-to-split-data-into-training-testing-sets-using-sample-function
## set the seed to make your partition reproducible
set.seed(311)

train_ind <- sample(seq_len(nrow(default_data_std)), size = floor(0.60 * nrow(default_data_std)))

train_data <- default_data_std[train_ind, ]
train_defaults <- defaults_outcome[train_ind]

test_data <- default_data_std[-train_ind, ]
test_defaults <- defaults_outcome[-train_ind]
```


<a name="knn"></a>

&nbsp;

## K-Nearest Neighbours In R

&nbsp;

After the preprocessing work, the `knn()` function in R can be used for implementing the K-Nearest Neighbours algorithm.

&nbsp;

```{r}
## KNN:
# Predict if there's a deafult based on balance and income 
# (not factoring in student)

library(class)

set.seed(101)
predicted_defaults <- knn(train_data, test_data, train_defaults, k = 1)

head(predicted_defaults)
```

Error rates can be determined by finding the percentage of cases where the predictions do not match the observations from the test set. This is done with the `mean()` function and the use of `!=`.


```{r}
# Compare observations from test set to predictions. This is the error rate with !=. 

mean(test_defaults != predicted_defaults)

# KNN With k = 3:

predicted_defaults <- knn(train_data, test_data, train_defaults, k = 3)

head(predicted_defaults)

mean(test_defaults != predicted_defaults)

# KNN With k = 5:

predicted_defaults <- knn(train_data, test_data, train_defaults, k = 5)

head(predicted_defaults)

mean(test_defaults != predicted_defaults)
```

<a name="optimal"></a>

&nbsp;

## Finding The Optimal Number Of K Neighbours

&nbsp;

In the KNN algorithm above, there was not much thought when it came to choosing the k-value in the `knn()` function. The numbers 1, 3 and 5 were used without much thought.

With the use of a for loop and elbow plot, we can visually determing which value(s) of k would be good/optimal in the KNN algorithm.


&nbsp;

```{r}
# For Loop approach for finding optimal k"

predicted_defaults = NULL
error_rate = NULL

for(i in 1:20){
  set.seed(101)
  predicted_defaults <- knn(train_data, test_data, train_defaults, k = i)
  error_rate[i] = mean(test_defaults != predicted_defaults)
}
```

&nbsp;

The error rates are stored in a data frame which is ready for plotting in `ggplot2`.

&nbsp;

```{r, echo = TRUE, fig.width=3.5, fig.height=3.5}
# Elbow Method Plot With ggplot2

library(ggplot2)

error_df <- data.frame(k = 1:20, error_rate)

error_df

# Elbow plot with ggplot2 (Looks like k = 6 is good)

ggplot(error_df, aes(x = k ,y = error_rate)) + 
  geom_point() + 
  geom_line(lty="dotted",color='red') +
  labs(x = "\n Number Of Neighbours (k)", y = "Error \n", 
     title = "K-Nearest Neighbours \n Selecting Optimal Value For k \n") + 
  theme(plot.title = element_text(hjust = 0.5), 
        axis.title.x = element_text(face="bold", colour = "darkgreen", size = 12),
        axis.title.y = element_text(face="bold", colour = "darkgreen", size = 12),
        legend.title = element_text(face="bold", size = 10))
```

&nbsp;

Good values for k include 6, 7 and 9.

